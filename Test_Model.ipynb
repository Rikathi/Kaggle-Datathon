{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "c1NdiUIGNXuz"
   },
   "outputs": [],
   "source": [
    "import spacy \n",
    "import nltk\n",
    "import re\n",
    "import cv2\n",
    "import pathlib\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_path = '/home/rikathipal/Rikathi/Indo-ML/IndoML_Datathon_2022/stopwords.txt'\n",
    "extracted_text_folder_path = '/home/rikathipal/Rikathi/Indo-ML/IndoML_Datathon_2022/text_data_validation'\n",
    "tokenizer_path = '/home/rikathipal/Desktop/models/tokenizer.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "vCqC0KS0J1pk"
   },
   "outputs": [],
   "source": [
    "def get_text_from_path(path):\n",
    "    with open(path) as f:\n",
    "        lines = f.readlines()\n",
    "        lines  = ' '.join(lines)\n",
    "        f.close()\n",
    "    return lines\n",
    "\n",
    "def get_corresponding_txtpath(img_path):\n",
    "    '''path str is the path of the folder containing txt file'''\n",
    "    path_str = extracted_text_folder_path\n",
    "    path_str = path_str + '/' + img_path.split('/')[-1].split('.')[0] + '.txt'    \n",
    "    return path_str\n",
    "\n",
    "def get_file_paths_and_labels(img_data_root):\n",
    "    img_paths = [str(path) for path in img_data_root.glob('**/*.tif')]\n",
    "    text_paths = [get_corresponding_txtpath(str(path)) for path in img_paths]\n",
    "    return img_paths, text_paths   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "id": "67aB5Qn6Mjab",
    "outputId": "783ec80b-9742-4c16-983d-8548a4aab37f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'count play home'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenize, Lemmatize, stopwords removal\n",
    "\n",
    "def preprocess(text_string):\n",
    "    preprocessed_string = re.sub(r'[^\\w\\s]','',text_string)\n",
    "    preprocessed_string = preprocessed_string.replace('\\n',' ')\n",
    "    preprocessed_string = preprocessed_string.replace('_',' ')\n",
    "    preprocessed_string = re.sub(' +', ' ', preprocessed_string)\n",
    "    return preprocessed_string\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\" )\n",
    "def get_stopwords(file_path):\n",
    "    with open(file_path, \"r\") as fp:\n",
    "        content = fp.read()\n",
    "        stops = content.split(\"\\n\")\n",
    "        stops = stops[:-1]\n",
    "        fp.close()\n",
    "        return stops\n",
    "\n",
    "stops = get_stopwords(stopwords_path)\n",
    "\n",
    "def normalize(comment, lowercase, remove_stopwords):\n",
    "    if lowercase:\n",
    "        comment = comment.lower()\n",
    "    comment = nlp(comment)\n",
    "    lemmatized = list()\n",
    "    for word in comment:\n",
    "        lemma = word.lemma_.strip()\n",
    "        if lemma:\n",
    "            if not remove_stopwords or (remove_stopwords and lemma not in stops):\n",
    "                lemmatized.append(lemma)\n",
    "    return \" \".join(lemmatized)\n",
    "\n",
    "normalize(\"counting playing the Home\", lowercase=True, remove_stopwords=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "ISOqB5JhNRuV"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "auto = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "def dataframe_to_dataset(dataframe):\n",
    "    d = {}\n",
    "    embed_tensor = []\n",
    "    for i in dataframe['texts_embedding']:\n",
    "        embed_tensor.append(tf.convert_to_tensor(i))\n",
    "\n",
    "    img_data_array = []\n",
    "    for this_path in dataframe['img_paths']:\n",
    "        image= cv2.imread(this_path, 0)\n",
    "        image=cv2.resize(image, (224, 224))\n",
    "        image=np.array(image)\n",
    "        image = image.astype(float)\n",
    "        image = np.stack((image,)*3, axis=-1)\n",
    "        image = tf.image.per_image_standardization(image)\n",
    "        img_data_array.append(tf.convert_to_tensor(image))\n",
    "\n",
    "    d['texts_embedding'] = embed_tensor\n",
    "    d['img_paths'] = img_data_array\n",
    "\n",
    "    ds = tf.data.Dataset.from_tensor_slices(d)\n",
    "    return ds\n",
    "\n",
    "def preprocess_text_and_image(sample):\n",
    "    image = sample[\"img_paths\"]\n",
    "    text = sample['texts_embedding']\n",
    "    return {\"image_inputs\": image,  \"text_inputs\": text}\n",
    "  \n",
    "\n",
    "def prepare_dataset(dataframe, training = True):\n",
    "    ds = dataframe_to_dataset(dataframe)\n",
    "    if training:\n",
    "        ds = ds.shuffle(len(df))\n",
    "    ds = ds.map(lambda x: preprocess_text_and_image(x)).cache()\n",
    "    ds = ds.batch(batch_size).prefetch(auto)\n",
    "    return ds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model_path, data_path):\n",
    "    # generates CSV contains id and labels  \n",
    "    img_data_root = pathlib.Path(data_path)\n",
    "    img_paths , text_paths =  get_file_paths_and_labels(img_data_root); \n",
    "    text_data = [get_text_from_path(path) for path in text_paths]\n",
    "    \n",
    "    df = pd.DataFrame(list(zip(text_paths, text_data, img_paths)),\n",
    "               columns =['text_paths','texts', 'img_paths'])\n",
    "    \n",
    "    df['texts'] = [preprocess(str(this_text)) for this_text in df['texts']]\n",
    "    df['texts'] = [normalize(this_text, lowercase=True, remove_stopwords=True) for this_text in df['texts']]\n",
    "    \n",
    "    with open(tokenizer_path, 'rb') as handle:\n",
    "        tokenizer = pickle.load(handle)\n",
    "        \n",
    "    max_len = 500 \n",
    "    \n",
    "    test_sequence = tokenizer.texts_to_sequences(df['texts'])\n",
    "    test_padded = pad_sequences(test_sequence, maxlen = max_len, truncating = \"post\", padding = \"post\" )\n",
    "\n",
    "    test_tensor = [tf.convert_to_tensor(test_padded[i]) for i in range(test_padded.shape[0])]\n",
    "    df['texts_embedding'] = test_tensor\n",
    "    \n",
    "    test_ds = prepare_dataset(df, False)\n",
    "    \n",
    "    loaded_model = keras.models.load_model(model_path)\n",
    "    \n",
    "    y_pred = loaded_model.predict(test_ds)\n",
    "    y_pred = [np.argmax(i) for i in y_pred]\n",
    "    \n",
    "    img_ids = [s.split(\"/\")[-1].split('.')[0] for s in df['img_paths']]\n",
    "    \n",
    "    submission_df = pd.DataFrame(list(zip(img_ids,y_pred)),columns = ['id','label'])\n",
    "    return submission_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function preprocess_text_and_image at 0x7f16c14d0b00> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function preprocess_text_and_image at 0x7f16c14d0b00> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "model_path = \"/home/rikathipal/Desktop/models/our_model.h5\"\n",
    "img_data_path = \"/home/rikathipal/Rikathi/Indo-ML/IndoML_Datathon_2022/validation\"\n",
    "submission_df = test(model_path, img_data_path)\n",
    "\n",
    "\n",
    "save_path = '/home/rikathipal/Desktop/models/submission.csv'\n",
    "submission_df.to_csv(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17888</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17801</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17891</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17802</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17889</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>18655</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>18656</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>18657</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>18658</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>18659</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>900 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  label\n",
       "0    17888     10\n",
       "1    17801      4\n",
       "2    17891     12\n",
       "3    17802      6\n",
       "4    17889     12\n",
       "..     ...    ...\n",
       "895  18655      4\n",
       "896  18656     11\n",
       "897  18657      6\n",
       "898  18658     15\n",
       "899  18659      5\n",
       "\n",
       "[900 rows x 2 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_paths</th>\n",
       "      <th>texts</th>\n",
       "      <th>img_paths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...</td>\n",
       "      <td>hemere wearn er 1 haren 24100 nanas enh ae le ...</td>\n",
       "      <td>/home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...</td>\n",
       "      <td>department health human service pilblic heath ...</td>\n",
       "      <td>/home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...</td>\n",
       "      <td>betsy imginnttnen sara 20x8 st aig 20x ad 0</td>\n",
       "      <td>/home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...</td>\n",
       "      <td>bagel new yory turspay janc ry ms front page 1...</td>\n",
       "      <td>/home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...</td>\n",
       "      <td>I 4 eben 2 tes ta wah photocopy complie us cop...</td>\n",
       "      <td>/home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          text_paths  \\\n",
       "0  /home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...   \n",
       "1  /home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...   \n",
       "2  /home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...   \n",
       "3  /home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...   \n",
       "4  /home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...   \n",
       "\n",
       "                                               texts  \\\n",
       "0  hemere wearn er 1 haren 24100 nanas enh ae le ...   \n",
       "1  department health human service pilblic heath ...   \n",
       "2        betsy imginnttnen sara 20x8 st aig 20x ad 0   \n",
       "3  bagel new yory turspay janc ry ms front page 1...   \n",
       "4  I 4 eben 2 tes ta wah photocopy complie us cop...   \n",
       "\n",
       "                                           img_paths  \n",
       "0  /home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...  \n",
       "1  /home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...  \n",
       "2  /home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...  \n",
       "3  /home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...  \n",
       "4  /home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_df = pd.read_csv('/home/rikathipal/Desktop/test_df.csv')\n",
    "t_df = t_df.drop(['Unnamed: 0', 'texts_embedding'], axis = 1)\n",
    "t_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/home/rikathipal/Desktop/models/our_model.h5'\n",
    "out = my_test(model_path, t_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6883</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14094</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9192</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12529</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14662</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>13230</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>13321</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>2646</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>6452</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599</th>\n",
       "      <td>13366</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  label\n",
       "0      6883      6\n",
       "1     14094      0\n",
       "2      9192      8\n",
       "3     12529      9\n",
       "4     14662      6\n",
       "...     ...    ...\n",
       "1595  13230      9\n",
       "1596  13321     15\n",
       "1597   2646      2\n",
       "1598   6452      6\n",
       "1599  13366      7\n",
       "\n",
       "[1600 rows x 2 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72875"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(test_ds_result, list(out['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_paths</th>\n",
       "      <th>texts</th>\n",
       "      <th>img_paths</th>\n",
       "      <th>texts_embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...</td>\n",
       "      <td>2500023668 6961 le 930 nal shimlzan shit vinid...</td>\n",
       "      <td>/home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...</td>\n",
       "      <td>(tf.Tensor(9630, shape=(), dtype=int32), tf.Te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...</td>\n",
       "      <td>780000347 780000317 site produce bw web</td>\n",
       "      <td>/home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...</td>\n",
       "      <td>(tf.Tensor(9679, shape=(), dtype=int32), tf.Te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...</td>\n",
       "      <td>persone seeger tee ss 200s 2000 rew atime hoa ...</td>\n",
       "      <td>/home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...</td>\n",
       "      <td>(tf.Tensor(4622, shape=(), dtype=int32), tf.Te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...</td>\n",
       "      <td>1820 inhalation sidestream cigarette smoke acc...</td>\n",
       "      <td>/home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...</td>\n",
       "      <td>(tf.Tensor(9694, shape=(), dtype=int32), tf.Te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...</td>\n",
       "      <td>medium release court find nhmrc act improperly...</td>\n",
       "      <td>/home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...</td>\n",
       "      <td>(tf.Tensor(426, shape=(), dtype=int32), tf.Ten...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          text_paths  \\\n",
       "0  /home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...   \n",
       "1  /home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...   \n",
       "2  /home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...   \n",
       "3  /home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...   \n",
       "4  /home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...   \n",
       "\n",
       "                                               texts  \\\n",
       "0  2500023668 6961 le 930 nal shimlzan shit vinid...   \n",
       "1            780000347 780000317 site produce bw web   \n",
       "2  persone seeger tee ss 200s 2000 rew atime hoa ...   \n",
       "3  1820 inhalation sidestream cigarette smoke acc...   \n",
       "4  medium release court find nhmrc act improperly...   \n",
       "\n",
       "                                           img_paths  \\\n",
       "0  /home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...   \n",
       "1  /home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...   \n",
       "2  /home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...   \n",
       "3  /home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...   \n",
       "4  /home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...   \n",
       "\n",
       "                                     texts_embedding  \n",
       "0  (tf.Tensor(9630, shape=(), dtype=int32), tf.Te...  \n",
       "1  (tf.Tensor(9679, shape=(), dtype=int32), tf.Te...  \n",
       "2  (tf.Tensor(4622, shape=(), dtype=int32), tf.Te...  \n",
       "3  (tf.Tensor(9694, shape=(), dtype=int32), tf.Te...  \n",
       "4  (tf.Tensor(426, shape=(), dtype=int32), tf.Ten...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-06 20:01:27.250280: W tensorflow/core/kernels/data/cache_dataset_ops.cc:757] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for i in test_ds:\n",
    "    tt = i['image_inputs']\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(128, 224, 224, 3), dtype=float64, numpy=\n",
       "array([[[[ 0.17608714,  0.17608714,  0.17608714],\n",
       "         [ 0.17608714,  0.17608714,  0.17608714],\n",
       "         [ 0.17608714,  0.17608714,  0.17608714],\n",
       "         ...,\n",
       "         [ 0.17608714,  0.17608714,  0.17608714],\n",
       "         [ 0.17608714,  0.17608714,  0.17608714],\n",
       "         [ 0.17608714,  0.17608714,  0.17608714]],\n",
       "\n",
       "        [[ 0.17608714,  0.17608714,  0.17608714],\n",
       "         [ 0.17608714,  0.17608714,  0.17608714],\n",
       "         [ 0.17608714,  0.17608714,  0.17608714],\n",
       "         ...,\n",
       "         [ 0.17608714,  0.17608714,  0.17608714],\n",
       "         [ 0.17608714,  0.17608714,  0.17608714],\n",
       "         [ 0.17608714,  0.17608714,  0.17608714]],\n",
       "\n",
       "        [[ 0.17608714,  0.17608714,  0.17608714],\n",
       "         [ 0.17608714,  0.17608714,  0.17608714],\n",
       "         [ 0.17608714,  0.17608714,  0.17608714],\n",
       "         ...,\n",
       "         [ 0.17608714,  0.17608714,  0.17608714],\n",
       "         [ 0.17608714,  0.17608714,  0.17608714],\n",
       "         [ 0.17608714,  0.17608714,  0.17608714]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.17608714,  0.17608714,  0.17608714],\n",
       "         [ 0.17608714,  0.17608714,  0.17608714],\n",
       "         [ 0.17608714,  0.17608714,  0.17608714],\n",
       "         ...,\n",
       "         [ 0.17608714,  0.17608714,  0.17608714],\n",
       "         [ 0.17608714,  0.17608714,  0.17608714],\n",
       "         [ 0.17608714,  0.17608714,  0.17608714]],\n",
       "\n",
       "        [[ 0.17608714,  0.17608714,  0.17608714],\n",
       "         [ 0.17608714,  0.17608714,  0.17608714],\n",
       "         [ 0.17608714,  0.17608714,  0.17608714],\n",
       "         ...,\n",
       "         [ 0.17608714,  0.17608714,  0.17608714],\n",
       "         [ 0.17608714,  0.17608714,  0.17608714],\n",
       "         [ 0.17608714,  0.17608714,  0.17608714]],\n",
       "\n",
       "        [[ 0.14026539,  0.14026539,  0.14026539],\n",
       "         [ 0.17608714,  0.17608714,  0.17608714],\n",
       "         [ 0.17608714,  0.17608714,  0.17608714],\n",
       "         ...,\n",
       "         [ 0.17608714,  0.17608714,  0.17608714],\n",
       "         [-0.18213031, -0.18213031, -0.18213031],\n",
       "         [ 0.17608714,  0.17608714,  0.17608714]]],\n",
       "\n",
       "\n",
       "       [[[ 0.42300591,  0.42300591,  0.42300591],\n",
       "         [ 0.42300591,  0.42300591,  0.42300591],\n",
       "         [ 0.42300591,  0.42300591,  0.42300591],\n",
       "         ...,\n",
       "         [ 0.42300591,  0.42300591,  0.42300591],\n",
       "         [ 0.42300591,  0.42300591,  0.42300591],\n",
       "         [ 0.42300591,  0.42300591,  0.42300591]],\n",
       "\n",
       "        [[ 0.42300591,  0.42300591,  0.42300591],\n",
       "         [ 0.42300591,  0.42300591,  0.42300591],\n",
       "         [ 0.42300591,  0.42300591,  0.42300591],\n",
       "         ...,\n",
       "         [ 0.42300591,  0.42300591,  0.42300591],\n",
       "         [ 0.42300591,  0.42300591,  0.42300591],\n",
       "         [ 0.42300591,  0.42300591,  0.42300591]],\n",
       "\n",
       "        [[ 0.42300591,  0.42300591,  0.42300591],\n",
       "         [ 0.42300591,  0.42300591,  0.42300591],\n",
       "         [ 0.42300591,  0.42300591,  0.42300591],\n",
       "         ...,\n",
       "         [ 0.42300591,  0.42300591,  0.42300591],\n",
       "         [ 0.42300591,  0.42300591,  0.42300591],\n",
       "         [ 0.42300591,  0.42300591,  0.42300591]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.42300591,  0.42300591,  0.42300591],\n",
       "         [ 0.42300591,  0.42300591,  0.42300591],\n",
       "         [ 0.42300591,  0.42300591,  0.42300591],\n",
       "         ...,\n",
       "         [ 0.42300591,  0.42300591,  0.42300591],\n",
       "         [ 0.42300591,  0.42300591,  0.42300591],\n",
       "         [ 0.42300591,  0.42300591,  0.42300591]],\n",
       "\n",
       "        [[ 0.42300591,  0.42300591,  0.42300591],\n",
       "         [ 0.42300591,  0.42300591,  0.42300591],\n",
       "         [ 0.42300591,  0.42300591,  0.42300591],\n",
       "         ...,\n",
       "         [ 0.42300591,  0.42300591,  0.42300591],\n",
       "         [ 0.42300591,  0.42300591,  0.42300591],\n",
       "         [ 0.42300591,  0.42300591,  0.42300591]],\n",
       "\n",
       "        [[ 0.42300591,  0.42300591,  0.42300591],\n",
       "         [ 0.42300591,  0.42300591,  0.42300591],\n",
       "         [ 0.42300591,  0.42300591,  0.42300591],\n",
       "         ...,\n",
       "         [ 0.42300591,  0.42300591,  0.42300591],\n",
       "         [ 0.42300591,  0.42300591,  0.42300591],\n",
       "         [ 0.42300591,  0.42300591,  0.42300591]]],\n",
       "\n",
       "\n",
       "       [[[ 0.27595389,  0.27595389,  0.27595389],\n",
       "         [ 0.27595389,  0.27595389,  0.27595389],\n",
       "         [ 0.27595389,  0.27595389,  0.27595389],\n",
       "         ...,\n",
       "         [ 0.27595389,  0.27595389,  0.27595389],\n",
       "         [ 0.27595389,  0.27595389,  0.27595389],\n",
       "         [ 0.27595389,  0.27595389,  0.27595389]],\n",
       "\n",
       "        [[ 0.27595389,  0.27595389,  0.27595389],\n",
       "         [ 0.27595389,  0.27595389,  0.27595389],\n",
       "         [ 0.27595389,  0.27595389,  0.27595389],\n",
       "         ...,\n",
       "         [ 0.27595389,  0.27595389,  0.27595389],\n",
       "         [ 0.27595389,  0.27595389,  0.27595389],\n",
       "         [ 0.27595389,  0.27595389,  0.27595389]],\n",
       "\n",
       "        [[ 0.27595389,  0.27595389,  0.27595389],\n",
       "         [ 0.27595389,  0.27595389,  0.27595389],\n",
       "         [ 0.27595389,  0.27595389,  0.27595389],\n",
       "         ...,\n",
       "         [ 0.27595389,  0.27595389,  0.27595389],\n",
       "         [ 0.27595389,  0.27595389,  0.27595389],\n",
       "         [ 0.27595389,  0.27595389,  0.27595389]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.27595389,  0.27595389,  0.27595389],\n",
       "         [ 0.27595389,  0.27595389,  0.27595389],\n",
       "         [ 0.27595389,  0.27595389,  0.27595389],\n",
       "         ...,\n",
       "         [ 0.27595389,  0.27595389,  0.27595389],\n",
       "         [ 0.27595389,  0.27595389,  0.27595389],\n",
       "         [ 0.27595389,  0.27595389,  0.27595389]],\n",
       "\n",
       "        [[ 0.27595389,  0.27595389,  0.27595389],\n",
       "         [ 0.27595389,  0.27595389,  0.27595389],\n",
       "         [ 0.27595389,  0.27595389,  0.27595389],\n",
       "         ...,\n",
       "         [ 0.27595389,  0.27595389,  0.27595389],\n",
       "         [ 0.27595389,  0.27595389,  0.27595389],\n",
       "         [ 0.27595389,  0.27595389,  0.27595389]],\n",
       "\n",
       "        [[ 0.27595389,  0.27595389,  0.27595389],\n",
       "         [ 0.27595389,  0.27595389,  0.27595389],\n",
       "         [ 0.27595389,  0.27595389,  0.27595389],\n",
       "         ...,\n",
       "         [ 0.27595389,  0.27595389,  0.27595389],\n",
       "         [ 0.27595389,  0.27595389,  0.27595389],\n",
       "         [ 0.27595389,  0.27595389,  0.27595389]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[ 0.36106184,  0.36106184,  0.36106184],\n",
       "         [ 0.36106184,  0.36106184,  0.36106184],\n",
       "         [ 0.36106184,  0.36106184,  0.36106184],\n",
       "         ...,\n",
       "         [ 0.36106184,  0.36106184,  0.36106184],\n",
       "         [ 0.36106184,  0.36106184,  0.36106184],\n",
       "         [ 0.36106184,  0.36106184,  0.36106184]],\n",
       "\n",
       "        [[ 0.36106184,  0.36106184,  0.36106184],\n",
       "         [ 0.36106184,  0.36106184,  0.36106184],\n",
       "         [ 0.36106184,  0.36106184,  0.36106184],\n",
       "         ...,\n",
       "         [ 0.36106184,  0.36106184,  0.36106184],\n",
       "         [ 0.36106184,  0.36106184,  0.36106184],\n",
       "         [ 0.36106184,  0.36106184,  0.36106184]],\n",
       "\n",
       "        [[ 0.36106184,  0.36106184,  0.36106184],\n",
       "         [ 0.36106184,  0.36106184,  0.36106184],\n",
       "         [ 0.36106184,  0.36106184,  0.36106184],\n",
       "         ...,\n",
       "         [ 0.36106184,  0.36106184,  0.36106184],\n",
       "         [ 0.36106184,  0.36106184,  0.36106184],\n",
       "         [ 0.36106184,  0.36106184,  0.36106184]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.36106184,  0.36106184,  0.36106184],\n",
       "         [ 0.36106184,  0.36106184,  0.36106184],\n",
       "         [ 0.36106184,  0.36106184,  0.36106184],\n",
       "         ...,\n",
       "         [ 0.36106184,  0.36106184,  0.36106184],\n",
       "         [ 0.36106184,  0.36106184,  0.36106184],\n",
       "         [ 0.36106184,  0.36106184,  0.36106184]],\n",
       "\n",
       "        [[ 0.36106184,  0.36106184,  0.36106184],\n",
       "         [ 0.36106184,  0.36106184,  0.36106184],\n",
       "         [ 0.36106184,  0.36106184,  0.36106184],\n",
       "         ...,\n",
       "         [ 0.36106184,  0.36106184,  0.36106184],\n",
       "         [ 0.36106184,  0.36106184,  0.36106184],\n",
       "         [ 0.36106184,  0.36106184,  0.36106184]],\n",
       "\n",
       "        [[ 0.36106184,  0.36106184,  0.36106184],\n",
       "         [ 0.36106184,  0.36106184,  0.36106184],\n",
       "         [ 0.36106184,  0.36106184,  0.36106184],\n",
       "         ...,\n",
       "         [ 0.36106184,  0.36106184,  0.36106184],\n",
       "         [-0.87916642, -0.87916642, -0.87916642],\n",
       "         [ 0.36106184,  0.36106184,  0.36106184]]],\n",
       "\n",
       "\n",
       "       [[[ 0.32501167,  0.32501167,  0.32501167],\n",
       "         [ 0.32501167,  0.32501167,  0.32501167],\n",
       "         [ 0.32501167,  0.32501167,  0.32501167],\n",
       "         ...,\n",
       "         [ 0.32501167,  0.32501167,  0.32501167],\n",
       "         [ 0.32501167,  0.32501167,  0.32501167],\n",
       "         [ 0.32501167,  0.32501167,  0.32501167]],\n",
       "\n",
       "        [[ 0.32501167,  0.32501167,  0.32501167],\n",
       "         [ 0.2675547 ,  0.2675547 ,  0.2675547 ],\n",
       "         [ 0.30585936,  0.30585936,  0.30585936],\n",
       "         ...,\n",
       "         [ 0.32501167,  0.32501167,  0.32501167],\n",
       "         [ 0.32501167,  0.32501167,  0.32501167],\n",
       "         [ 0.32501167,  0.32501167,  0.32501167]],\n",
       "\n",
       "        [[ 0.32501167,  0.32501167,  0.32501167],\n",
       "         [ 0.32501167,  0.32501167,  0.32501167],\n",
       "         [-0.84327984, -0.84327984, -0.84327984],\n",
       "         ...,\n",
       "         [ 0.32501167,  0.32501167,  0.32501167],\n",
       "         [ 0.32501167,  0.32501167,  0.32501167],\n",
       "         [ 0.32501167,  0.32501167,  0.32501167]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.32501167,  0.32501167,  0.32501167],\n",
       "         [ 0.32501167,  0.32501167,  0.32501167],\n",
       "         [ 0.32501167,  0.32501167,  0.32501167],\n",
       "         ...,\n",
       "         [ 0.32501167,  0.32501167,  0.32501167],\n",
       "         [ 0.32501167,  0.32501167,  0.32501167],\n",
       "         [ 0.32501167,  0.32501167,  0.32501167]],\n",
       "\n",
       "        [[ 0.32501167,  0.32501167,  0.32501167],\n",
       "         [ 0.32501167,  0.32501167,  0.32501167],\n",
       "         [ 0.32501167,  0.32501167,  0.32501167],\n",
       "         ...,\n",
       "         [ 0.32501167,  0.32501167,  0.32501167],\n",
       "         [ 0.32501167,  0.32501167,  0.32501167],\n",
       "         [ 0.32501167,  0.32501167,  0.32501167]],\n",
       "\n",
       "        [[ 0.32501167,  0.32501167,  0.32501167],\n",
       "         [ 0.32501167,  0.32501167,  0.32501167],\n",
       "         [ 0.32501167,  0.32501167,  0.32501167],\n",
       "         ...,\n",
       "         [ 0.32501167,  0.32501167,  0.32501167],\n",
       "         [ 0.2675547 ,  0.2675547 ,  0.2675547 ],\n",
       "         [ 0.32501167,  0.32501167,  0.32501167]]],\n",
       "\n",
       "\n",
       "       [[[ 0.5535531 ,  0.5535531 ,  0.5535531 ],\n",
       "         [ 0.5535531 ,  0.5535531 ,  0.5535531 ],\n",
       "         [ 0.5535531 ,  0.5535531 ,  0.5535531 ],\n",
       "         ...,\n",
       "         [ 0.5535531 ,  0.5535531 ,  0.5535531 ],\n",
       "         [ 0.5535531 ,  0.5535531 ,  0.5535531 ],\n",
       "         [ 0.5535531 ,  0.5535531 ,  0.5535531 ]],\n",
       "\n",
       "        [[ 0.5535531 ,  0.5535531 ,  0.5535531 ],\n",
       "         [ 0.5535531 ,  0.5535531 ,  0.5535531 ],\n",
       "         [ 0.5535531 ,  0.5535531 ,  0.5535531 ],\n",
       "         ...,\n",
       "         [ 0.5535531 ,  0.5535531 ,  0.5535531 ],\n",
       "         [ 0.5535531 ,  0.5535531 ,  0.5535531 ],\n",
       "         [ 0.5535531 ,  0.5535531 ,  0.5535531 ]],\n",
       "\n",
       "        [[ 0.5535531 ,  0.5535531 ,  0.5535531 ],\n",
       "         [ 0.5535531 ,  0.5535531 ,  0.5535531 ],\n",
       "         [ 0.5535531 ,  0.5535531 ,  0.5535531 ],\n",
       "         ...,\n",
       "         [ 0.5535531 ,  0.5535531 ,  0.5535531 ],\n",
       "         [ 0.5535531 ,  0.5535531 ,  0.5535531 ],\n",
       "         [ 0.5535531 ,  0.5535531 ,  0.5535531 ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.5535531 ,  0.5535531 ,  0.5535531 ],\n",
       "         [ 0.5535531 ,  0.5535531 ,  0.5535531 ],\n",
       "         [ 0.5535531 ,  0.5535531 ,  0.5535531 ],\n",
       "         ...,\n",
       "         [ 0.5535531 ,  0.5535531 ,  0.5535531 ],\n",
       "         [ 0.5535531 ,  0.5535531 ,  0.5535531 ],\n",
       "         [ 0.5535531 ,  0.5535531 ,  0.5535531 ]],\n",
       "\n",
       "        [[ 0.5535531 ,  0.5535531 ,  0.5535531 ],\n",
       "         [ 0.5535531 ,  0.5535531 ,  0.5535531 ],\n",
       "         [ 0.5535531 ,  0.5535531 ,  0.5535531 ],\n",
       "         ...,\n",
       "         [ 0.5535531 ,  0.5535531 ,  0.5535531 ],\n",
       "         [ 0.5535531 ,  0.5535531 ,  0.5535531 ],\n",
       "         [ 0.5535531 ,  0.5535531 ,  0.5535531 ]],\n",
       "\n",
       "        [[ 0.5535531 ,  0.5535531 ,  0.5535531 ],\n",
       "         [ 0.5535531 ,  0.5535531 ,  0.5535531 ],\n",
       "         [ 0.5535531 ,  0.5535531 ,  0.5535531 ],\n",
       "         ...,\n",
       "         [ 0.5535531 ,  0.5535531 ,  0.5535531 ],\n",
       "         [ 0.02386237,  0.02386237,  0.02386237],\n",
       "         [ 0.5535531 ,  0.5535531 ,  0.5535531 ]]]])>"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from tensorflow.keras.optimizers import Adamax\n",
    "loaded_model = keras.models.load_model('/home/rikathipal/Desktop/models/g_v_model.h5')\n",
    "loaded_model.compile(Adamax(learning_rate=0.001), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 4, 1, 6, 5, 4, 3, 10, 3, 8, 12, 8, 3, 3, 4, 12, 3, 3, 1, 8, 1, 5, 4, 5, 12, 3, 1, 6, 12, 1, 1, 1, 6, 8, 5, 4, 6, 3, 4, 1, 0, 3, 8, 1, 11, 13, 5, 5, 8, 5, 5, 12, 12, 14, 10, 8, 9, 6, 3, 1, 4, 12, 1, 12, 5, 3, 8, 0, 5, 5, 3, 6, 6, 4, 11, 4, 1, 4, 0, 14, 12, 9, 0, 8, 5, 5, 8, 15, 5, 0, 1, 4, 8, 8, 1, 1, 8, 8, 8, 8, 8, 3, 5, 3, 3, 3, 4, 12, 4, 15, 11, 3, 8, 3, 5, 1, 11, 6, 8, 7, 8, 5, 1, 5, 5, 1, 12, 1, 3, 6, 4, 5, 10, 8, 1, 3, 4, 3, 6, 11, 1, 9, 5, 5, 1, 6, 0, 9, 0, 5, 5, 1, 6, 13, 8, 5, 8, 10, 4, 12, 3, 1, 5, 2, 1, 3, 8, 1, 9, 8, 5, 1, 4, 5, 4, 6, 3, 1, 0, 5, 0, 12, 10, 1, 1, 4, 3, 9, 0, 10, 11, 12, 0, 12, 8, 1, 6, 1, 8, 3, 11, 3, 12, 3, 1, 8, 5, 8, 5, 9, 8, 4, 6, 5, 3, 0, 6, 0, 3, 10, 6, 8, 8, 8, 1, 1, 3, 5, 0, 12, 1, 10, 9, 11, 5, 3, 5, 4, 4, 8, 1, 9, 8, 6, 3, 3, 4, 3, 3, 1, 3, 11, 1, 8, 1, 3, 11, 6, 3, 8, 8, 3, 5, 14, 4, 4, 8, 3, 1, 6, 12, 5, 4, 1, 14, 5, 9, 6, 3, 11, 12, 9, 8, 4, 12, 4, 3, 4, 1, 1, 1, 3, 1, 4, 1, 3, 1, 1, 6, 4, 1, 5, 1, 12, 12, 10, 1, 8, 12, 8, 13, 11, 5, 10, 8, 12, 5, 5, 12, 8, 6, 1, 1, 4, 6, 8, 3, 11, 4, 11, 1, 6, 1, 6, 11, 3, 9, 14, 6, 5, 5, 15, 8, 12, 5, 11, 3, 12, 12, 11, 1, 8, 1, 3, 11, 4, 4, 15, 12, 6, 4, 3, 14, 10, 4, 1, 8, 6, 1, 3, 10, 6, 3, 8, 4, 12, 10, 9, 1, 8, 0, 4, 12, 5, 12, 12, 5, 8, 8, 6, 5, 3, 12, 3, 8, 1, 5, 1, 6, 3, 3, 11, 12, 1, 1, 1, 1, 12, 3, 15, 8, 9, 1, 10, 0, 6, 8, 1, 6, 8, 3, 10, 10, 9, 10, 3, 1, 4, 12, 11, 10, 4, 12, 10, 10, 12, 3, 3, 1, 1, 5, 3, 4, 11, 6, 8, 8, 12, 5, 1, 1, 6, 10, 1, 1, 1, 0, 4, 1, 1, 8, 1, 11, 4, 5, 3, 1, 3, 8, 5, 1, 4, 1, 4, 1, 4, 8, 4, 9, 8, 12, 12, 10, 3, 4, 4, 5, 4, 4, 10, 3, 15, 8, 0, 8, 1, 4, 5, 3, 3, 12, 12, 4, 10, 5, 6, 12, 5, 1, 10, 5, 6, 9, 8, 5, 3, 5, 1, 8, 3, 1, 11, 5, 10, 4, 15, 8, 9, 1, 12, 8, 10, 4, 8, 1, 5, 4, 9, 6, 5, 12, 8, 1, 8, 8, 8, 0, 1, 1, 10, 3, 8, 4, 0, 5, 3, 4, 6, 1, 15, 1, 9, 5, 4, 3, 12, 10, 4, 9, 11, 9, 3, 9, 6, 8, 5, 8, 13, 1, 3, 9, 8, 4, 8, 1, 3, 6, 8, 8, 2, 1, 1, 1, 3, 8, 8, 8, 3, 4, 8, 3, 9, 3, 11, 1, 5, 8, 4, 10, 1, 5, 8, 1, 4, 1, 6, 12, 9, 1, 11, 3, 13, 8, 1, 3, 3, 8, 8, 12, 3, 4, 3, 11, 1, 1, 4, 1, 8, 12, 3, 3, 6, 3, 8, 1, 5, 5, 1, 1, 15, 3, 12, 5, 5, 8, 3, 1, 3, 4, 1, 14, 5, 11, 4, 12, 6, 11, 4, 10, 5, 1, 0, 12, 0, 5, 3, 12, 10, 1, 6, 3, 8, 5, 14, 1, 6, 3, 1, 3, 5, 12, 1, 8, 6, 12, 5, 8, 8, 8, 5, 10, 12, 8, 8, 5, 12, 4, 3, 1, 4, 1, 1, 12, 8, 6, 3, 5, 5, 11, 1, 1, 8, 12, 3, 1, 0, 12, 1, 1, 6, 1, 14, 1, 6, 3, 1, 1, 1, 0, 1, 9, 12, 10, 8, 4, 15, 8, 13, 1, 12, 1, 4, 12, 12, 6, 3, 6, 8, 5, 8, 2, 4, 4, 3, 1, 1, 6, 6, 11, 10, 12, 3, 1, 3, 9, 4, 10, 3, 1, 6, 1, 11, 3, 11, 4, 1, 1, 12, 8, 3, 11, 3, 11, 8, 0, 3, 4, 3, 8, 3, 5, 1, 1, 12, 4, 10, 15, 3, 10, 3, 3, 3, 12, 3, 3, 1, 10, 4, 8, 14, 4, 12, 5, 4, 5, 1, 8, 5, 8, 1, 6, 3, 3, 11, 6, 1, 5, 12, 3, 5, 6, 12, 10, 10, 8, 9, 10, 14, 8, 6, 4, 4, 5, 14, 3, 6, 6, 3, 3, 6, 5, 4, 5, 1, 6, 12, 8, 1, 4, 9, 8, 1, 8, 1, 0, 4, 10, 13, 3, 4, 8, 4, 6, 1, 4, 3, 12, 1, 8, 5, 1, 2, 5, 8, 7, 3, 9, 14, 5, 10]\n"
     ]
    }
   ],
   "source": [
    "y_pred = loaded_model.predict(test_ds)\n",
    "y_pred = [np.argmax(i) for i in y_pred]\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'17888'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_ids = [s.split(\"/\")[-1].split('.')[0] for s in df['img_paths']]\n",
    "img_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17888</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17801</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17891</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17802</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17889</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>18655</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>18656</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>18657</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>18658</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>18659</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>900 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  label\n",
       "0    17888     12\n",
       "1    17801      4\n",
       "2    17891      1\n",
       "3    17802      6\n",
       "4    17889     12\n",
       "..     ...    ...\n",
       "895  18655     13\n",
       "896  18656     12\n",
       "897  18657      6\n",
       "898  18658      1\n",
       "899  18659     10\n",
       "\n",
       "[900 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df = pd.DataFrame(list(zip(img_ids,y_pred)),columns = ['id','label'])\n",
    "submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv('/home/rikathipal/Desktop/models/submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text_paths</th>\n",
       "      <th>texts</th>\n",
       "      <th>img_paths</th>\n",
       "      <th>data_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>/home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...</td>\n",
       "      <td>r harritan company inc 2541 space road po box ...</td>\n",
       "      <td>/home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>/home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...</td>\n",
       "      <td>81180353</td>\n",
       "      <td>/home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>/home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...</td>\n",
       "      <td>sgent k group ltd 209e4e9230 052200 844 job 82...</td>\n",
       "      <td>/home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>/home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...</td>\n",
       "      <td>ctrl 43987 1994 congress cell tis9ue research ...</td>\n",
       "      <td>/home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>/home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...</td>\n",
       "      <td>gasiasy gyihl lsvoahod tvionvnis 66e vsn slo 3...</td>\n",
       "      <td>/home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                         text_paths  \\\n",
       "0           0  /home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...   \n",
       "1           1  /home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...   \n",
       "2           2  /home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...   \n",
       "3           3  /home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...   \n",
       "4           4  /home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...   \n",
       "\n",
       "                                               texts  \\\n",
       "0  r harritan company inc 2541 space road po box ...   \n",
       "1                                           81180353   \n",
       "2  sgent k group ltd 209e4e9230 052200 844 job 82...   \n",
       "3  ctrl 43987 1994 congress cell tis9ue research ...   \n",
       "4  gasiasy gyihl lsvoahod tvionvnis 66e vsn slo 3...   \n",
       "\n",
       "                                           img_paths  data_label  \n",
       "0  /home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...           1  \n",
       "1  /home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...           8  \n",
       "2  /home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...          13  \n",
       "3  /home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...           8  \n",
       "4  /home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...          10  "
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_df = pd.read_csv('/home/rikathipal/Rikathi/merged_df.csv')\n",
    "t_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        r harritan company inc 2541 space road po box ...\n",
       "1                                                 81180353\n",
       "2        sgent k group ltd 209e4e9230 052200 844 job 82...\n",
       "3        ctrl 43987 1994 congress cell tis9ue research ...\n",
       "4        gasiasy gyihl lsvoahod tvionvnis 66e vsn slo 3...\n",
       "                               ...                        \n",
       "15995    environment international vol 12 9939 1986 pri...\n",
       "15996    e tabac reunie sa moaernestabasesi abrique di ...\n",
       "15997                                           f99zelpz0z\n",
       "15998    einean toon billing instruction wail vour invo...\n",
       "15999    principal investigatoriprogram director charle...\n",
       "Name: texts, Length: 16000, dtype: object"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_df['texts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(t_df, test_size=0.2, random_state=42)\n",
    "test_df, val_df = train_test_split(test_df, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.texts = test_df.texts.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 500 \n",
    "# Fit the tokenizer\n",
    "tokenizer = Tokenizer(num_words = 65000)\n",
    "tokenizer.fit_on_texts(test_df['texts'])\n",
    "\n",
    "# sequence the input corpus and add zero padding upto 500 word\n",
    "test_sequence = tokenizer.texts_to_sequences(test_df['texts'])\n",
    "test_padded = pad_sequences(test_sequence, maxlen = max_len, truncating = \"post\", padding = \"post\" )\n",
    "\n",
    "\n",
    "test_tensor = [tf.convert_to_tensor(test_padded[i]) for i in range(test_padded.shape[0])]\n",
    "test_df['texts_embedding'] = test_tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text_paths</th>\n",
       "      <th>texts</th>\n",
       "      <th>img_paths</th>\n",
       "      <th>data_label</th>\n",
       "      <th>texts_embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12507</th>\n",
       "      <td>12507</td>\n",
       "      <td>/home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...</td>\n",
       "      <td>hemere wearn er 1 haren 24100 nanas enh ae le ...</td>\n",
       "      <td>/home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...</td>\n",
       "      <td>6</td>\n",
       "      <td>(tf.Tensor(14805, shape=(), dtype=int32), tf.T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4564</th>\n",
       "      <td>4564</td>\n",
       "      <td>/home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...</td>\n",
       "      <td>department health human service pilblic heath ...</td>\n",
       "      <td>/home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...</td>\n",
       "      <td>0</td>\n",
       "      <td>(tf.Tensor(57, shape=(), dtype=int32), tf.Tens...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15043</th>\n",
       "      <td>15043</td>\n",
       "      <td>/home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...</td>\n",
       "      <td>betsy imginnttnen sara 20x8 st aig 20x ad 0</td>\n",
       "      <td>/home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...</td>\n",
       "      <td>8</td>\n",
       "      <td>(tf.Tensor(6079, shape=(), dtype=int32), tf.Te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2714</th>\n",
       "      <td>2714</td>\n",
       "      <td>/home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...</td>\n",
       "      <td>bagel new yory turspay janc ry ms front page 1...</td>\n",
       "      <td>/home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...</td>\n",
       "      <td>9</td>\n",
       "      <td>(tf.Tensor(9694, shape=(), dtype=int32), tf.Te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5206</th>\n",
       "      <td>5206</td>\n",
       "      <td>/home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...</td>\n",
       "      <td>I 4 eben 2 tes ta wah photocopy complie us cop...</td>\n",
       "      <td>/home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...</td>\n",
       "      <td>6</td>\n",
       "      <td>(tf.Tensor(1, shape=(), dtype=int32), tf.Tenso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3545</th>\n",
       "      <td>3545</td>\n",
       "      <td>/home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...</td>\n",
       "      <td>jstonauburn I journal 014256 ewtow metropolita...</td>\n",
       "      <td>/home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...</td>\n",
       "      <td>9</td>\n",
       "      <td>(tf.Tensor(54766, shape=(), dtype=int32), tf.T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3733</th>\n",
       "      <td>3733</td>\n",
       "      <td>/home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...</td>\n",
       "      <td>philip morris management corp interoffice corr...</td>\n",
       "      <td>/home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...</td>\n",
       "      <td>15</td>\n",
       "      <td>(tf.Tensor(55, shape=(), dtype=int32), tf.Tens...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7770</th>\n",
       "      <td>7770</td>\n",
       "      <td>/home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...</td>\n",
       "      <td>accord analysis page 1of 1 jaalelelxl el ea pl...</td>\n",
       "      <td>/home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...</td>\n",
       "      <td>2</td>\n",
       "      <td>(tf.Tensor(1303, shape=(), dtype=int32), tf.Te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11972</th>\n",
       "      <td>11972</td>\n",
       "      <td>/home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...</td>\n",
       "      <td>1a orkid eben nn ae vggrath ibe lat bc beresio...</td>\n",
       "      <td>/home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...</td>\n",
       "      <td>6</td>\n",
       "      <td>(tf.Tensor(1740, shape=(), dtype=int32), tf.Te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3703</th>\n",
       "      <td>3703</td>\n",
       "      <td>/home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...</td>\n",
       "      <td>loursvrite basic weight specification x virgin...</td>\n",
       "      <td>/home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...</td>\n",
       "      <td>7</td>\n",
       "      <td>(tf.Tensor(9943, shape=(), dtype=int32), tf.Te...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                         text_paths  \\\n",
       "12507       12507  /home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...   \n",
       "4564         4564  /home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...   \n",
       "15043       15043  /home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...   \n",
       "2714         2714  /home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...   \n",
       "5206         5206  /home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...   \n",
       "...           ...                                                ...   \n",
       "3545         3545  /home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...   \n",
       "3733         3733  /home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...   \n",
       "7770         7770  /home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...   \n",
       "11972       11972  /home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...   \n",
       "3703         3703  /home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...   \n",
       "\n",
       "                                                   texts  \\\n",
       "12507  hemere wearn er 1 haren 24100 nanas enh ae le ...   \n",
       "4564   department health human service pilblic heath ...   \n",
       "15043        betsy imginnttnen sara 20x8 st aig 20x ad 0   \n",
       "2714   bagel new yory turspay janc ry ms front page 1...   \n",
       "5206   I 4 eben 2 tes ta wah photocopy complie us cop...   \n",
       "...                                                  ...   \n",
       "3545   jstonauburn I journal 014256 ewtow metropolita...   \n",
       "3733   philip morris management corp interoffice corr...   \n",
       "7770   accord analysis page 1of 1 jaalelelxl el ea pl...   \n",
       "11972  1a orkid eben nn ae vggrath ibe lat bc beresio...   \n",
       "3703   loursvrite basic weight specification x virgin...   \n",
       "\n",
       "                                               img_paths  data_label  \\\n",
       "12507  /home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...           6   \n",
       "4564   /home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...           0   \n",
       "15043  /home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...           8   \n",
       "2714   /home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...           9   \n",
       "5206   /home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...           6   \n",
       "...                                                  ...         ...   \n",
       "3545   /home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...           9   \n",
       "3733   /home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...          15   \n",
       "7770   /home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...           2   \n",
       "11972  /home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...           6   \n",
       "3703   /home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...           7   \n",
       "\n",
       "                                         texts_embedding  \n",
       "12507  (tf.Tensor(14805, shape=(), dtype=int32), tf.T...  \n",
       "4564   (tf.Tensor(57, shape=(), dtype=int32), tf.Tens...  \n",
       "15043  (tf.Tensor(6079, shape=(), dtype=int32), tf.Te...  \n",
       "2714   (tf.Tensor(9694, shape=(), dtype=int32), tf.Te...  \n",
       "5206   (tf.Tensor(1, shape=(), dtype=int32), tf.Tenso...  \n",
       "...                                                  ...  \n",
       "3545   (tf.Tensor(54766, shape=(), dtype=int32), tf.T...  \n",
       "3733   (tf.Tensor(55, shape=(), dtype=int32), tf.Tens...  \n",
       "7770   (tf.Tensor(1303, shape=(), dtype=int32), tf.Te...  \n",
       "11972  (tf.Tensor(1740, shape=(), dtype=int32), tf.Te...  \n",
       "3703   (tf.Tensor(9943, shape=(), dtype=int32), tf.Te...  \n",
       "\n",
       "[1600 rows x 6 columns]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds_labels = test_df['data_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_paths</th>\n",
       "      <th>texts</th>\n",
       "      <th>img_paths</th>\n",
       "      <th>texts_embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12507</th>\n",
       "      <td>/home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...</td>\n",
       "      <td>hemere wearn er 1 haren 24100 nanas enh ae le ...</td>\n",
       "      <td>/home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...</td>\n",
       "      <td>(tf.Tensor(14805, shape=(), dtype=int32), tf.T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4564</th>\n",
       "      <td>/home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...</td>\n",
       "      <td>department health human service pilblic heath ...</td>\n",
       "      <td>/home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...</td>\n",
       "      <td>(tf.Tensor(57, shape=(), dtype=int32), tf.Tens...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15043</th>\n",
       "      <td>/home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...</td>\n",
       "      <td>betsy imginnttnen sara 20x8 st aig 20x ad 0</td>\n",
       "      <td>/home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...</td>\n",
       "      <td>(tf.Tensor(6079, shape=(), dtype=int32), tf.Te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2714</th>\n",
       "      <td>/home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...</td>\n",
       "      <td>bagel new yory turspay janc ry ms front page 1...</td>\n",
       "      <td>/home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...</td>\n",
       "      <td>(tf.Tensor(9694, shape=(), dtype=int32), tf.Te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5206</th>\n",
       "      <td>/home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...</td>\n",
       "      <td>I 4 eben 2 tes ta wah photocopy complie us cop...</td>\n",
       "      <td>/home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...</td>\n",
       "      <td>(tf.Tensor(1, shape=(), dtype=int32), tf.Tenso...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              text_paths  \\\n",
       "12507  /home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...   \n",
       "4564   /home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...   \n",
       "15043  /home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...   \n",
       "2714   /home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...   \n",
       "5206   /home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...   \n",
       "\n",
       "                                                   texts  \\\n",
       "12507  hemere wearn er 1 haren 24100 nanas enh ae le ...   \n",
       "4564   department health human service pilblic heath ...   \n",
       "15043        betsy imginnttnen sara 20x8 st aig 20x ad 0   \n",
       "2714   bagel new yory turspay janc ry ms front page 1...   \n",
       "5206   I 4 eben 2 tes ta wah photocopy complie us cop...   \n",
       "\n",
       "                                               img_paths  \\\n",
       "12507  /home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...   \n",
       "4564   /home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...   \n",
       "15043  /home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...   \n",
       "2714   /home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...   \n",
       "5206   /home/rikathipal/Rikathi/Indo-ML/IndoML_Datath...   \n",
       "\n",
       "                                         texts_embedding  \n",
       "12507  (tf.Tensor(14805, shape=(), dtype=int32), tf.T...  \n",
       "4564   (tf.Tensor(57, shape=(), dtype=int32), tf.Tens...  \n",
       "15043  (tf.Tensor(6079, shape=(), dtype=int32), tf.Te...  \n",
       "2714   (tf.Tensor(9694, shape=(), dtype=int32), tf.Te...  \n",
       "5206   (tf.Tensor(1, shape=(), dtype=int32), tf.Tenso...  "
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_df_wo_label = test_df.drop(['data_label', r'Unnamed: 0'], axis =1 )\n",
    "t_df_wo_label.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = prepare_dataset(t_df_wo_label, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image_inputs': <tf.Tensor: shape=(128, 224, 224, 3), dtype=float64, numpy=\n",
      "array([[[[0.44550028, 0.44550028, 0.44550028],\n",
      "         [0.44550028, 0.44550028, 0.44550028],\n",
      "         [0.44550028, 0.44550028, 0.44550028],\n",
      "         ...,\n",
      "         [0.44550028, 0.44550028, 0.44550028],\n",
      "         [0.44550028, 0.44550028, 0.44550028],\n",
      "         [0.44550028, 0.44550028, 0.44550028]],\n",
      "\n",
      "        [[0.44550028, 0.44550028, 0.44550028],\n",
      "         [0.44550028, 0.44550028, 0.44550028],\n",
      "         [0.44550028, 0.44550028, 0.44550028],\n",
      "         ...,\n",
      "         [0.44550028, 0.44550028, 0.44550028],\n",
      "         [0.44550028, 0.44550028, 0.44550028],\n",
      "         [0.44550028, 0.44550028, 0.44550028]],\n",
      "\n",
      "        [[0.44550028, 0.44550028, 0.44550028],\n",
      "         [0.44550028, 0.44550028, 0.44550028],\n",
      "         [0.44550028, 0.44550028, 0.44550028],\n",
      "         ...,\n",
      "         [0.44550028, 0.44550028, 0.44550028],\n",
      "         [0.44550028, 0.44550028, 0.44550028],\n",
      "         [0.44550028, 0.44550028, 0.44550028]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.44550028, 0.44550028, 0.44550028],\n",
      "         [0.44550028, 0.44550028, 0.44550028],\n",
      "         [0.44550028, 0.44550028, 0.44550028],\n",
      "         ...,\n",
      "         [0.44550028, 0.44550028, 0.44550028],\n",
      "         [0.44550028, 0.44550028, 0.44550028],\n",
      "         [0.44550028, 0.44550028, 0.44550028]],\n",
      "\n",
      "        [[0.44550028, 0.44550028, 0.44550028],\n",
      "         [0.44550028, 0.44550028, 0.44550028],\n",
      "         [0.44550028, 0.44550028, 0.44550028],\n",
      "         ...,\n",
      "         [0.44550028, 0.44550028, 0.44550028],\n",
      "         [0.44550028, 0.44550028, 0.44550028],\n",
      "         [0.44550028, 0.44550028, 0.44550028]],\n",
      "\n",
      "        [[0.44550028, 0.44550028, 0.44550028],\n",
      "         [0.44550028, 0.44550028, 0.44550028],\n",
      "         [0.44550028, 0.44550028, 0.44550028],\n",
      "         ...,\n",
      "         [0.44550028, 0.44550028, 0.44550028],\n",
      "         [0.44550028, 0.44550028, 0.44550028],\n",
      "         [0.44550028, 0.44550028, 0.44550028]]],\n",
      "\n",
      "\n",
      "       [[[0.27185267, 0.27185267, 0.27185267],\n",
      "         [0.27185267, 0.27185267, 0.27185267],\n",
      "         [0.27185267, 0.27185267, 0.27185267],\n",
      "         ...,\n",
      "         [0.27185267, 0.27185267, 0.27185267],\n",
      "         [0.27185267, 0.27185267, 0.27185267],\n",
      "         [0.27185267, 0.27185267, 0.27185267]],\n",
      "\n",
      "        [[0.27185267, 0.27185267, 0.27185267],\n",
      "         [0.27185267, 0.27185267, 0.27185267],\n",
      "         [0.27185267, 0.27185267, 0.27185267],\n",
      "         ...,\n",
      "         [0.27185267, 0.27185267, 0.27185267],\n",
      "         [0.27185267, 0.27185267, 0.27185267],\n",
      "         [0.27185267, 0.27185267, 0.27185267]],\n",
      "\n",
      "        [[0.27185267, 0.27185267, 0.27185267],\n",
      "         [0.27185267, 0.27185267, 0.27185267],\n",
      "         [0.27185267, 0.27185267, 0.27185267],\n",
      "         ...,\n",
      "         [0.27185267, 0.27185267, 0.27185267],\n",
      "         [0.27185267, 0.27185267, 0.27185267],\n",
      "         [0.27185267, 0.27185267, 0.27185267]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.27185267, 0.27185267, 0.27185267],\n",
      "         [0.27185267, 0.27185267, 0.27185267],\n",
      "         [0.27185267, 0.27185267, 0.27185267],\n",
      "         ...,\n",
      "         [0.27185267, 0.27185267, 0.27185267],\n",
      "         [0.27185267, 0.27185267, 0.27185267],\n",
      "         [0.27185267, 0.27185267, 0.27185267]],\n",
      "\n",
      "        [[0.27185267, 0.27185267, 0.27185267],\n",
      "         [0.27185267, 0.27185267, 0.27185267],\n",
      "         [0.27185267, 0.27185267, 0.27185267],\n",
      "         ...,\n",
      "         [0.27185267, 0.27185267, 0.27185267],\n",
      "         [0.27185267, 0.27185267, 0.27185267],\n",
      "         [0.27185267, 0.27185267, 0.27185267]],\n",
      "\n",
      "        [[0.27185267, 0.27185267, 0.27185267],\n",
      "         [0.27185267, 0.27185267, 0.27185267],\n",
      "         [0.27185267, 0.27185267, 0.27185267],\n",
      "         ...,\n",
      "         [0.27185267, 0.27185267, 0.27185267],\n",
      "         [0.27185267, 0.27185267, 0.27185267],\n",
      "         [0.27185267, 0.27185267, 0.27185267]]],\n",
      "\n",
      "\n",
      "       [[[0.47304487, 0.47304487, 0.47304487],\n",
      "         [0.47304487, 0.47304487, 0.47304487],\n",
      "         [0.47304487, 0.47304487, 0.47304487],\n",
      "         ...,\n",
      "         [0.47304487, 0.47304487, 0.47304487],\n",
      "         [0.47304487, 0.47304487, 0.47304487],\n",
      "         [0.47304487, 0.47304487, 0.47304487]],\n",
      "\n",
      "        [[0.47304487, 0.47304487, 0.47304487],\n",
      "         [0.47304487, 0.47304487, 0.47304487],\n",
      "         [0.47304487, 0.47304487, 0.47304487],\n",
      "         ...,\n",
      "         [0.47304487, 0.47304487, 0.47304487],\n",
      "         [0.47304487, 0.47304487, 0.47304487],\n",
      "         [0.47304487, 0.47304487, 0.47304487]],\n",
      "\n",
      "        [[0.47304487, 0.47304487, 0.47304487],\n",
      "         [0.47304487, 0.47304487, 0.47304487],\n",
      "         [0.47304487, 0.47304487, 0.47304487],\n",
      "         ...,\n",
      "         [0.47304487, 0.47304487, 0.47304487],\n",
      "         [0.47304487, 0.47304487, 0.47304487],\n",
      "         [0.47304487, 0.47304487, 0.47304487]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.47304487, 0.47304487, 0.47304487],\n",
      "         [0.47304487, 0.47304487, 0.47304487],\n",
      "         [0.47304487, 0.47304487, 0.47304487],\n",
      "         ...,\n",
      "         [0.47304487, 0.47304487, 0.47304487],\n",
      "         [0.47304487, 0.47304487, 0.47304487],\n",
      "         [0.47304487, 0.47304487, 0.47304487]],\n",
      "\n",
      "        [[0.47304487, 0.47304487, 0.47304487],\n",
      "         [0.47304487, 0.47304487, 0.47304487],\n",
      "         [0.47304487, 0.47304487, 0.47304487],\n",
      "         ...,\n",
      "         [0.47304487, 0.47304487, 0.47304487],\n",
      "         [0.47304487, 0.47304487, 0.47304487],\n",
      "         [0.47304487, 0.47304487, 0.47304487]],\n",
      "\n",
      "        [[0.47304487, 0.47304487, 0.47304487],\n",
      "         [0.47304487, 0.47304487, 0.47304487],\n",
      "         [0.47304487, 0.47304487, 0.47304487],\n",
      "         ...,\n",
      "         [0.47304487, 0.47304487, 0.47304487],\n",
      "         [0.47304487, 0.47304487, 0.47304487],\n",
      "         [0.47304487, 0.47304487, 0.47304487]]],\n",
      "\n",
      "\n",
      "       ...,\n",
      "\n",
      "\n",
      "       [[[0.3323054 , 0.3323054 , 0.3323054 ],\n",
      "         [0.3323054 , 0.3323054 , 0.3323054 ],\n",
      "         [0.3323054 , 0.3323054 , 0.3323054 ],\n",
      "         ...,\n",
      "         [0.3323054 , 0.3323054 , 0.3323054 ],\n",
      "         [0.3323054 , 0.3323054 , 0.3323054 ],\n",
      "         [0.3323054 , 0.3323054 , 0.3323054 ]],\n",
      "\n",
      "        [[0.3323054 , 0.3323054 , 0.3323054 ],\n",
      "         [0.3323054 , 0.3323054 , 0.3323054 ],\n",
      "         [0.3323054 , 0.3323054 , 0.3323054 ],\n",
      "         ...,\n",
      "         [0.3323054 , 0.3323054 , 0.3323054 ],\n",
      "         [0.3323054 , 0.3323054 , 0.3323054 ],\n",
      "         [0.3323054 , 0.3323054 , 0.3323054 ]],\n",
      "\n",
      "        [[0.3323054 , 0.3323054 , 0.3323054 ],\n",
      "         [0.3323054 , 0.3323054 , 0.3323054 ],\n",
      "         [0.3323054 , 0.3323054 , 0.3323054 ],\n",
      "         ...,\n",
      "         [0.3323054 , 0.3323054 , 0.3323054 ],\n",
      "         [0.3323054 , 0.3323054 , 0.3323054 ],\n",
      "         [0.3323054 , 0.3323054 , 0.3323054 ]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.3323054 , 0.3323054 , 0.3323054 ],\n",
      "         [0.3323054 , 0.3323054 , 0.3323054 ],\n",
      "         [0.3323054 , 0.3323054 , 0.3323054 ],\n",
      "         ...,\n",
      "         [0.3323054 , 0.3323054 , 0.3323054 ],\n",
      "         [0.3323054 , 0.3323054 , 0.3323054 ],\n",
      "         [0.3323054 , 0.3323054 , 0.3323054 ]],\n",
      "\n",
      "        [[0.3323054 , 0.3323054 , 0.3323054 ],\n",
      "         [0.3323054 , 0.3323054 , 0.3323054 ],\n",
      "         [0.3323054 , 0.3323054 , 0.3323054 ],\n",
      "         ...,\n",
      "         [0.3323054 , 0.3323054 , 0.3323054 ],\n",
      "         [0.3323054 , 0.3323054 , 0.3323054 ],\n",
      "         [0.3323054 , 0.3323054 , 0.3323054 ]],\n",
      "\n",
      "        [[0.3323054 , 0.3323054 , 0.3323054 ],\n",
      "         [0.3323054 , 0.3323054 , 0.3323054 ],\n",
      "         [0.3323054 , 0.3323054 , 0.3323054 ],\n",
      "         ...,\n",
      "         [0.3323054 , 0.3323054 , 0.3323054 ],\n",
      "         [0.3323054 , 0.3323054 , 0.3323054 ],\n",
      "         [0.3323054 , 0.3323054 , 0.3323054 ]]],\n",
      "\n",
      "\n",
      "       [[[0.46146077, 0.46146077, 0.46146077],\n",
      "         [0.46146077, 0.46146077, 0.46146077],\n",
      "         [0.46146077, 0.46146077, 0.46146077],\n",
      "         ...,\n",
      "         [0.46146077, 0.46146077, 0.46146077],\n",
      "         [0.46146077, 0.46146077, 0.46146077],\n",
      "         [0.46146077, 0.46146077, 0.46146077]],\n",
      "\n",
      "        [[0.39927089, 0.39927089, 0.39927089],\n",
      "         [0.46146077, 0.46146077, 0.46146077],\n",
      "         [0.46146077, 0.46146077, 0.46146077],\n",
      "         ...,\n",
      "         [0.46146077, 0.46146077, 0.46146077],\n",
      "         [0.46146077, 0.46146077, 0.46146077],\n",
      "         [0.46146077, 0.46146077, 0.46146077]],\n",
      "\n",
      "        [[0.46146077, 0.46146077, 0.46146077],\n",
      "         [0.46146077, 0.46146077, 0.46146077],\n",
      "         [0.46146077, 0.46146077, 0.46146077],\n",
      "         ...,\n",
      "         [0.46146077, 0.46146077, 0.46146077],\n",
      "         [0.46146077, 0.46146077, 0.46146077],\n",
      "         [0.46146077, 0.46146077, 0.46146077]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.46146077, 0.46146077, 0.46146077],\n",
      "         [0.46146077, 0.46146077, 0.46146077],\n",
      "         [0.46146077, 0.46146077, 0.46146077],\n",
      "         ...,\n",
      "         [0.46146077, 0.46146077, 0.46146077],\n",
      "         [0.46146077, 0.46146077, 0.46146077],\n",
      "         [0.46146077, 0.46146077, 0.46146077]],\n",
      "\n",
      "        [[0.46146077, 0.46146077, 0.46146077],\n",
      "         [0.46146077, 0.46146077, 0.46146077],\n",
      "         [0.46146077, 0.46146077, 0.46146077],\n",
      "         ...,\n",
      "         [0.46146077, 0.46146077, 0.46146077],\n",
      "         [0.46146077, 0.46146077, 0.46146077],\n",
      "         [0.46146077, 0.46146077, 0.46146077]],\n",
      "\n",
      "        [[0.46146077, 0.46146077, 0.46146077],\n",
      "         [0.46146077, 0.46146077, 0.46146077],\n",
      "         [0.46146077, 0.46146077, 0.46146077],\n",
      "         ...,\n",
      "         [0.46146077, 0.46146077, 0.46146077],\n",
      "         [0.46146077, 0.46146077, 0.46146077],\n",
      "         [0.46146077, 0.46146077, 0.46146077]]],\n",
      "\n",
      "\n",
      "       [[[0.10829835, 0.10829835, 0.10829835],\n",
      "         [0.10829835, 0.10829835, 0.10829835],\n",
      "         [0.10829835, 0.10829835, 0.10829835],\n",
      "         ...,\n",
      "         [0.10829835, 0.10829835, 0.10829835],\n",
      "         [0.10829835, 0.10829835, 0.10829835],\n",
      "         [0.10829835, 0.10829835, 0.10829835]],\n",
      "\n",
      "        [[0.10829835, 0.10829835, 0.10829835],\n",
      "         [0.10829835, 0.10829835, 0.10829835],\n",
      "         [0.10829835, 0.10829835, 0.10829835],\n",
      "         ...,\n",
      "         [0.10829835, 0.10829835, 0.10829835],\n",
      "         [0.10829835, 0.10829835, 0.10829835],\n",
      "         [0.10829835, 0.10829835, 0.10829835]],\n",
      "\n",
      "        [[0.10829835, 0.10829835, 0.10829835],\n",
      "         [0.10829835, 0.10829835, 0.10829835],\n",
      "         [0.10829835, 0.10829835, 0.10829835],\n",
      "         ...,\n",
      "         [0.10829835, 0.10829835, 0.10829835],\n",
      "         [0.10829835, 0.10829835, 0.10829835],\n",
      "         [0.10829835, 0.10829835, 0.10829835]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.10829835, 0.10829835, 0.10829835],\n",
      "         [0.10829835, 0.10829835, 0.10829835],\n",
      "         [0.10829835, 0.10829835, 0.10829835],\n",
      "         ...,\n",
      "         [0.10829835, 0.10829835, 0.10829835],\n",
      "         [0.10829835, 0.10829835, 0.10829835],\n",
      "         [0.10829835, 0.10829835, 0.10829835]],\n",
      "\n",
      "        [[0.10829835, 0.10829835, 0.10829835],\n",
      "         [0.10829835, 0.10829835, 0.10829835],\n",
      "         [0.10829835, 0.10829835, 0.10829835],\n",
      "         ...,\n",
      "         [0.10829835, 0.10829835, 0.10829835],\n",
      "         [0.10829835, 0.10829835, 0.10829835],\n",
      "         [0.10829835, 0.10829835, 0.10829835]],\n",
      "\n",
      "        [[0.10829835, 0.10829835, 0.10829835],\n",
      "         [0.10829835, 0.10829835, 0.10829835],\n",
      "         [0.10829835, 0.10829835, 0.10829835],\n",
      "         ...,\n",
      "         [0.10829835, 0.10829835, 0.10829835],\n",
      "         [0.10829835, 0.10829835, 0.10829835],\n",
      "         [0.10829835, 0.10829835, 0.10829835]]]])>, 'text_inputs': <tf.Tensor: shape=(128, 500), dtype=int32, numpy=\n",
      "array([[14805, 14806,   229, ...,     0,     0,     0],\n",
      "       [   57,    48,   119, ...,     0,     0,     0],\n",
      "       [ 6079, 14937,  9692, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [  896,    51,  4731, ...,     0,     0,     0],\n",
      "       [ 4192,  4734, 18237, ...,     0,     0,     0],\n",
      "       [18339, 10422,   474, ...,     0,     0,     0]], dtype=int32)>}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-06 20:02:23.911680: W tensorflow/core/kernels/data/cache_dataset_ops.cc:757] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for i in test_ds:\n",
    "    print(i); break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from tensorflow.keras.optimizers import Adamax\n",
    "loaded_model = keras.models.load_model('/home/rikathipal/Desktop/models/g_v_model.h5')\n",
    "loaded_model.compile(Adamax(learning_rate=0.001), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = loaded_model.predict(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 8, 2, 6, 2, 2, 8, 14, 14, 2, 5, 8, 2, 2, 2, 5, 2, 8, 8, 5, 12, 8, 8, 2, 2, 2, 2, 8, 8, 2, 2, 2, 12, 2, 2, 14, 2, 2, 6, 2, 2, 8, 2, 8, 2, 8, 8, 6, 2, 2, 12, 2, 2, 8, 2, 5, 2, 2, 2, 5, 2, 2, 4, 2, 4, 2, 8, 2, 2, 8, 8, 2, 8, 2, 5, 8, 5, 5, 2, 8, 2, 2, 2, 8, 2, 11, 2, 2, 2, 8, 8, 2, 2, 8, 2, 2, 2, 8, 8, 2, 2, 8, 5, 2, 2, 2, 2, 14, 2, 2, 10, 8, 2, 2, 2, 10, 2, 5, 2, 12, 6, 8, 5, 14, 2, 6, 5, 2, 2, 2, 2, 5, 5, 2, 5, 2, 2, 8, 2, 8, 0, 2, 13, 2, 8, 2, 8, 2, 2, 2, 2, 2, 2, 2, 2, 2, 8, 2, 2, 14, 11, 2, 2, 2, 6, 5, 2, 2, 8, 2, 6, 8, 2, 2, 8, 2, 2, 2, 2, 13, 2, 2, 2, 2, 2, 2, 2, 2, 2, 8, 5, 2, 8, 2, 5, 14, 2, 2, 8, 5, 2, 2, 2, 2, 8, 2, 5, 8, 2, 2, 2, 2, 2, 2, 14, 6, 2, 5, 5, 2, 8, 14, 5, 5, 8, 8, 2, 8, 8, 2, 2, 9, 2, 8, 5, 2, 8, 12, 5, 2, 13, 13, 2, 2, 12, 2, 2, 6, 2, 14, 14, 8, 2, 2, 8, 10, 5, 2, 2, 2, 2, 5, 5, 5, 2, 5, 8, 8, 5, 2, 6, 12, 2, 8, 2, 10, 2, 2, 2, 2, 2, 14, 14, 8, 2, 8, 2, 2, 5, 14, 12, 2, 2, 2, 4, 2, 2, 14, 12, 2, 2, 5, 2, 11, 5, 2, 13, 8, 8, 2, 8, 2, 13, 2, 2, 2, 0, 2, 2, 2, 2, 12, 14, 2, 2, 2, 2, 8, 8, 2, 6, 2, 8, 8, 6, 2, 8, 2, 14, 14, 6, 8, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 8, 2, 8, 2, 2, 8, 2, 2, 2, 2, 2, 2, 2, 10, 8, 5, 12, 2, 2, 5, 2, 2, 2, 5, 2, 2, 2, 2, 2, 12, 2, 5, 5, 2, 8, 2, 2, 5, 2, 2, 8, 2, 2, 4, 8, 2, 2, 2, 6, 8, 14, 8, 2, 2, 5, 8, 8, 10, 2, 12, 8, 2, 12, 2, 2, 2, 2, 8, 2, 8, 5, 8, 6, 2, 2, 2, 8, 8, 8, 8, 8, 2, 8, 2, 2, 8, 5, 8, 8, 2, 8, 2, 2, 14, 2, 2, 2, 2, 14, 2, 2, 8, 2, 8, 8, 2, 6, 2, 2, 6, 2, 5, 2, 2, 13, 2, 6, 2, 2, 2, 2, 13, 2, 2, 2, 10, 2, 2, 2, 2, 2, 2, 8, 2, 2, 2, 2, 2, 8, 11, 2, 2, 2, 2, 5, 2, 2, 8, 9, 2, 0, 12, 6, 2, 2, 2, 2, 8, 2, 2, 5, 2, 12, 6, 8, 2, 2, 8, 5, 2, 8, 2, 2, 6, 8, 2, 2, 2, 2, 2, 2, 2, 5, 2, 8, 2, 8, 2, 8, 2, 14, 5, 8, 5, 2, 2, 2, 8, 2, 11, 8, 5, 5, 8, 5, 6, 2, 2, 2, 8, 8, 5, 2, 2, 2, 2, 2, 8, 14, 2, 2, 6, 2, 5, 12, 2, 5, 5, 5, 5, 8, 2, 2, 2, 2, 2, 0, 2, 2, 2, 8, 14, 12, 5, 8, 12, 8, 2, 8, 6, 5, 5, 4, 8, 2, 2, 2, 14, 8, 2, 2, 2, 0, 2, 8, 2, 12, 8, 10, 5, 2, 2, 2, 2, 2, 12, 5, 12, 8, 8, 2, 2, 2, 8, 14, 5, 2, 8, 8, 14, 8, 10, 2, 5, 8, 8, 2, 2, 5, 2, 8, 8, 2, 2, 9, 8, 2, 2, 2, 2, 13, 8, 2, 2, 2, 2, 2, 2, 5, 2, 5, 2, 13, 8, 2, 5, 0, 2, 2, 6, 2, 2, 8, 8, 2, 5, 14, 2, 2, 2, 2, 2, 8, 2, 8, 2, 2, 5, 14, 8, 8, 2, 0, 2, 2, 8, 2, 5, 13, 2, 8, 2, 2, 2, 8, 2, 2, 14, 10, 2, 5, 8, 8, 2, 2, 8, 2, 14, 2, 8, 8, 2, 2, 10, 2, 8, 8, 6, 8, 2, 2, 2, 2, 8, 7, 2, 2, 2, 8, 2, 2, 8, 2, 8, 2, 14, 14, 2, 8, 0, 2, 2, 5, 14, 2, 6, 2, 2, 5, 2, 2, 2, 14, 2, 2, 2, 5, 2, 2, 2, 2, 2, 2, 2, 14, 8, 8, 8, 2, 8, 2, 8, 8, 2, 8, 8, 2, 2, 2, 2, 6, 8, 14, 2, 2, 2, 2, 8, 8, 8, 2, 8, 2, 2, 8, 2, 2, 2, 8, 2, 8, 2, 2, 5, 8, 8, 2, 2, 2, 2, 8, 8, 2, 2, 2, 2, 11, 8, 6, 6, 8, 8, 8, 2, 2, 8, 13, 8, 8, 6, 8, 2, 2, 2, 2, 2, 2, 2, 2, 5, 2, 2, 8, 2, 11, 2, 9, 2, 2, 2, 14, 2, 6, 2, 11, 2, 2, 2, 2, 2, 2, 5, 2, 14, 8, 2, 2, 2, 6, 14, 2, 6, 5, 5, 2, 8, 2, 8, 0, 2, 2, 2, 2, 2, 5, 2, 2, 2, 5, 5, 2, 8, 6, 2, 2, 7, 2, 2, 5, 14, 8, 2, 2, 2, 2, 2, 8, 13, 8, 2, 8, 0, 2, 5, 5, 2, 2, 8, 8, 2, 8, 2, 2, 8, 5, 2, 2, 8, 2, 5, 6, 2, 12, 10, 8, 5, 8, 2, 2, 2, 8, 2, 10, 2, 2, 5, 8, 5, 2, 8, 2, 2, 12, 2, 2, 2, 12, 8, 2, 2, 2, 2, 2, 6, 2, 5, 2, 5, 8, 6, 8, 8, 2, 6, 2, 2, 2, 2, 2, 8, 12, 5, 2, 2, 2, 8, 10, 2, 2, 8, 5, 8, 12, 2, 14, 2, 2, 2, 2, 13, 2, 6, 2, 2, 8, 2, 2, 2, 8, 5, 8, 2, 6, 2, 8, 2, 12, 2, 14, 8, 2, 2, 8, 2, 2, 2, 10, 6, 8, 6, 2, 2, 2, 8, 2, 2, 2, 2, 2, 8, 2, 8, 2, 2, 2, 2, 2, 2, 12, 2, 8, 2, 8, 2, 6, 2, 2, 2, 8, 5, 2, 2, 2, 2, 2, 8, 2, 8, 2, 5, 8, 2, 8, 2, 6, 8, 14, 8, 2, 2, 4, 2, 2, 6, 8, 2, 2, 2, 2, 12, 2, 12, 2, 2, 2, 8, 2, 8, 2, 2, 2, 12, 2, 8, 2, 14, 2, 8, 2, 2, 2, 8, 8, 2, 2, 8, 2, 13, 2, 2, 10, 2, 8, 8, 5, 8, 0, 8, 2, 2, 5, 4, 8, 2, 8, 2, 2, 2, 7, 5, 12, 8, 2, 2, 5, 12, 8, 6, 2, 2, 2, 8, 2, 5, 2, 2, 2, 8, 2, 2, 2, 14, 2, 2, 12, 8, 2, 8, 2, 6, 14, 5, 2, 8, 6, 2, 2, 8, 2, 2, 2, 2, 2, 2, 11, 2, 2, 2, 6, 8, 8, 8, 8, 2, 5, 5, 2, 6, 8, 2, 2, 8, 2, 2, 2, 8, 2, 8, 2, 2, 8, 2, 2, 2, 8, 2, 2, 8, 2, 2, 2, 13, 2, 2, 2, 12, 2, 2, 2, 8, 2, 2, 13, 2, 2, 2, 2, 0, 8, 5, 8, 2, 14, 2, 2, 8, 2, 2, 5, 6, 8, 14, 5, 2, 8, 8, 6, 2, 2, 2, 2, 12, 2, 2, 11, 2, 14, 2, 2, 2, 2, 2, 2, 2, 8, 2, 2, 2, 8, 2, 2, 12, 2, 2, 5, 9, 8, 12, 5, 8, 2, 8, 2, 2, 2, 2, 5, 2, 2, 2, 13, 2, 2, 4, 2, 2, 8, 8, 8, 8, 2, 2, 8, 2, 2, 2, 2, 2, 2, 5, 2, 2, 8, 2, 2, 5, 2, 2, 8, 2, 2, 2, 2, 8, 2, 5, 2, 0, 6, 5, 2, 8, 2, 2, 2, 2, 8, 2, 8, 2, 2, 8, 2, 2, 2, 2, 2, 2, 2, 8, 2, 5, 2, 2, 2, 8, 2, 8, 2, 2, 2, 2, 8, 2, 2, 2, 9, 2, 2, 8, 8, 2, 2, 10, 2, 2, 12, 2, 2, 2, 2, 11, 5, 8, 5, 2, 8, 12, 2, 8, 8, 2, 8, 2, 2, 10, 2, 2, 5, 2, 5, 2, 8, 2, 12, 2, 5, 2, 2, 8, 13, 5, 2, 6, 2, 2, 2, 2, 2, 2, 5, 8, 2, 2, 12, 2, 2, 5, 6, 2, 8, 6, 5, 2, 2, 8, 5, 5, 8, 2, 8, 2, 11, 2, 2, 2, 2, 5, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 8, 2, 2, 5, 12, 2, 2, 2, 2, 8, 2, 12, 2, 8, 2, 2, 5, 5, 8, 2, 2, 2, 2, 6, 2, 8, 8, 8, 2, 2, 8, 8, 8, 8, 2, 2, 2, 5, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 4, 5, 2, 5, 5, 2, 12, 5, 8, 8, 8, 8, 10, 2, 8, 2, 2, 14, 2, 2, 2, 8, 5, 6, 13, 8, 2, 8, 2, 2, 14, 5, 2, 5, 6, 2, 2, 5, 2, 8, 2, 2, 0, 2, 2, 2, 8, 8, 8, 5, 2, 2, 2, 5, 2, 5, 5, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "y_pred = [np.argmax(i) for i in y_pred]\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13625"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "accuracy_score(test_ds_labels, y_pred)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
